{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import load_patient, extract_training_windows\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "#_______________________________________________\n",
    "# Initializations\n",
    "#_______________________________________________\n",
    "\n",
    "# Window/Segment length \n",
    "l = 20 #seconds\n",
    "# window stride for testing. \n",
    "s = 10 #seconds\n",
    "\n",
    "# Base folder path\n",
    "base_path = '../'\n",
    "\n",
    "training_data_path = base_path + 'training_data/'\n",
    "\n",
    "#_______________________________________________\n",
    "# Dont change These Values\n",
    "#_______________________________________________\n",
    "# Sapmling frequency of ecg signal is 400 hz. (CPSC2020 Data)\n",
    "fs = 400\n",
    "# Window/Segment length in samples. \n",
    "win_size = l*fs\n",
    "# Stride for test window in samples. \n",
    "stride = s*fs\n",
    "#_______________________________________________\n",
    "\n",
    "if not os.path.exists(training_data_path):\n",
    "    os.makedirs(training_data_path)\n",
    "    \n",
    "for pat_num in range(1,11):\n",
    "\n",
    "    #_______________________________________________\n",
    "    # Training Data Preparation\n",
    "    #_______________________________________________\n",
    "    # Load 1 patient ecg and annotations\n",
    "    ecg, R_ann, S_ann, V_ann = load_patient(base_path, pat_num)\n",
    "    X_train, y_train, R_w, S_w, V_w = extract_training_windows(ecg, R_ann, S_ann, V_ann, win_size)\n",
    "\n",
    "    print('Total Windows : ', len(X_train))\n",
    "\n",
    "    # Indexes of windows where V beats are present.\n",
    "    s_w_idx = []\n",
    "    for idx,ann in enumerate(S_w):    \n",
    "        if ann.any():\n",
    "            s_w_idx.append(idx)\n",
    "    s_w_idx = np.asarray(s_w_idx, dtype=np.int32)\n",
    "\n",
    "    # Indexes of windows where V beats are present.\n",
    "    v_w_idx = []\n",
    "    for idx,ann in enumerate(V_w):    \n",
    "        if ann.any():\n",
    "            v_w_idx.append(idx)\n",
    "    v_w_idx = np.asarray(v_w_idx, dtype=np.int32)\n",
    "\n",
    "\n",
    "    # Indexes of windows for V beats and S beats combined.\n",
    "    sv_idx = np.unique(np.concatenate((s_w_idx,v_w_idx)))\n",
    "\n",
    "    # All indexes of training windows.\n",
    "    idx = np.arange(len(X_train))\n",
    "\n",
    "    # indexes other than S and V beats. (Normal R peaks)\n",
    "    rem_idx = np.delete(idx, sv_idx, 0)\n",
    "\n",
    "    # Choose 50% of remaining. \n",
    "    norm_idx = np.random.choice(rem_idx, size=int(len(rem_idx)*(1/3)), replace=False)\n",
    "\n",
    "    X_train = np.concatenate((X_train[norm_idx],X_train[sv_idx]))\n",
    "    y_train = np.concatenate((y_train[norm_idx],y_train[sv_idx]))\n",
    "\n",
    "    assert len(X_train) == len(y_train)\n",
    "\n",
    "    print('Selected Windows : ', len(X_train))\n",
    "\n",
    "    print('Saving Data')\n",
    "    f_X = training_data_path+ 'X_train_P' + str(pat_num).zfill(2) + '.npy'\n",
    "    f_y = training_data_path+ 'y_train_P' + str(pat_num).zfill(2) + '.npy'\n",
    "    np.save(f_X, X_train)\n",
    "    np.save(f_y, y_train)\n",
    "    print('Done..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import load_patient, extract_training_windows, get_noise,normalize_bound\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#_______________________________________________\n",
    "# Initializations\n",
    "#_______________________________________________\n",
    "\n",
    "# Window/Segment length \n",
    "l = 20 #seconds\n",
    "# window stride for testing. \n",
    "s = 10 #seconds\n",
    "\n",
    "# Base folder path\n",
    "base_path = '../'\n",
    "\n",
    "training_data_path = base_path + 'training_data_aug/'\n",
    "\n",
    "#_______________________________________________\n",
    "# Dont change These Values\n",
    "#_______________________________________________\n",
    "# Sapmling frequency of ecg signal is 400 hz. (CPSC2020 Data)\n",
    "fs = 400\n",
    "# Window/Segment length in samples. \n",
    "win_size = l*fs\n",
    "# Stride for test window in samples. \n",
    "stride = s*fs\n",
    "#_______________________________________________\n",
    "\n",
    "if not os.path.exists(training_data_path):\n",
    "    os.makedirs(training_data_path)\n",
    "\n",
    "\n",
    "\n",
    "ma = np.load(base_path +'data/ma.npy')\n",
    "bw = np.load(base_path +'data/bw.npy')\n",
    "\n",
    "\n",
    "aug_factor = 1 # How many times to augment.\n",
    "\n",
    "\n",
    "\n",
    "for pat_num in range(1,11):\n",
    "\n",
    "\n",
    "    #_______________________________________________\n",
    "    # Training Data Preparation\n",
    "    #_______________________________________________\n",
    "    # Load 1 patient ecg and annotations\n",
    "    ecg, R_ann, S_ann, V_ann = load_patient(base_path, pat_num)\n",
    "    X_train, y_train, R_w, S_w, V_w = extract_training_windows(ecg, R_ann, S_ann, V_ann, win_size)\n",
    "\n",
    "    print('Total Windows : ', len(X_train))\n",
    "\n",
    "    # Indexes of windows where S beats are present.\n",
    "    s_w_idx = []\n",
    "    for idx,ann in enumerate(S_w):    \n",
    "        if ann.any():\n",
    "            s_w_idx.append(idx)\n",
    "    s_w_idx = np.asarray(s_w_idx, dtype=np.int32)\n",
    "\n",
    "\n",
    "    # Indexes of windows where V beats are present.\n",
    "    v_w_idx = []\n",
    "    for idx,ann in enumerate(V_w):    \n",
    "        if ann.any():\n",
    "            v_w_idx.append(idx)\n",
    "    v_w_idx = np.asarray(v_w_idx, dtype=np.int32)\n",
    "\n",
    "    # Indexes of windows for V beats and S beats combined.\n",
    "    sv_idx = np.unique(np.concatenate((s_w_idx,v_w_idx)))\n",
    "    # All indexes of training windows.\n",
    "    idx = np.arange(len(X_train))\n",
    "    # indexes other than S and V beats. (Normal R peaks)\n",
    "    rem_idx = np.delete(idx, sv_idx, 0)\n",
    "    \n",
    "    #_____________________________________________________________________\n",
    "    # Choose 50% of remaining. \n",
    "    #norm_idx = np.random.choice(rem_idx, size=int(len(rem_idx)*(1/2)), replace=False)\n",
    "    \n",
    "    norm_idx = rem_idx[::2]\n",
    "    \n",
    "    # Placeholder for augmented beats\n",
    "    X_aug = np.zeros((int(len(sv_idx)*aug_factor), win_size))\n",
    "    y_aug = np.zeros((int(len(sv_idx)*aug_factor), win_size))\n",
    "    \n",
    "    count = 0\n",
    "    for i in tqdm(range(len(sv_idx))):\n",
    "\n",
    "        current_window = X_train[sv_idx[i]]\n",
    "        current_window = np.squeeze(current_window)\n",
    "\n",
    "        for j in range(aug_factor):\n",
    "\n",
    "            noise = get_noise(ma, bw, win_size)\n",
    "            aug_window = current_window + noise\n",
    "\n",
    "            X_aug[count] = normalize_bound(aug_window, lb=-1, ub=1)\n",
    "\n",
    "            y_aug[count] = np.squeeze(y_train[sv_idx[i]])\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "    X_aug = np.expand_dims(X_aug, axis=2)\n",
    "    y_aug = np.expand_dims(y_aug, axis=2)\n",
    "    \n",
    "    \n",
    "    #X_train = np.concatenate((X_train,X_aug))\n",
    "    #y_train = np.concatenate((y_train,y_aug))\n",
    "    \n",
    "    X_train = np.concatenate((X_train[norm_idx],X_train[sv_idx], X_aug))\n",
    "    y_train = np.concatenate((y_train[norm_idx],y_train[sv_idx], y_aug))\n",
    "\n",
    "\n",
    "    assert len(X_train) == len(y_train)\n",
    "\n",
    "    print('Normal Beat Windows : ', len(rem_idx))\n",
    "    print('Abnormal Beat Windows : ', len(sv_idx))\n",
    "    print('Augmented abnormal Beat Windows : ', len(X_aug))\n",
    "\n",
    "    print('Saving Data')\n",
    "    f_X = training_data_path+ 'X_train_P' + str(pat_num).zfill(2) + '.npy'\n",
    "    f_y = training_data_path+ 'y_train_P' + str(pat_num).zfill(2) + '.npy'\n",
    "    np.save(f_X, X_train)\n",
    "    np.save(f_y, y_train)\n",
    "    print('Done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________\n",
      "Run :  1\n",
      "______________________________________________\n",
      "______________________________________________\n",
      "Loading Data for Patient : 1\n",
      "______________________________________________\n",
      "Total Beats :  109731\n",
      "S beats :  24\n",
      "V beats :  0\n",
      "______________________________________________\n",
      "Predicting Test Patient....\n",
      "______________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110285/110285 [01:05<00:00, 1686.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109539\n",
      "______________________________________________\n",
      "All Beats\n",
      "______________________________________________\n",
      "_________Calculating Stats____________________\n",
      "______________________________________________\n",
      "TP's:109402 FN's:329 FP's:116\n",
      "Recall:99.7, Precision(FNR):99.89, F1-Score:99.79\n",
      "Total 109731\n",
      "______________________________________________\n",
      "S Beats\n",
      "______________________________________________\n",
      "_________Calculating Stats____________________\n",
      "______________________________________________\n",
      "TP's:24 FN's:0 FP's:96617\n",
      "Recall:100.0, Precision(FNR):0.02, F1-Score:0.04\n",
      "Total 24\n",
      "______________________________________________\n",
      "Loading Data for Patient : 2\n",
      "______________________________________________\n",
      "Total Beats :  108297\n",
      "S beats :  0\n",
      "V beats :  4554\n",
      "______________________________________________\n",
      "Predicting Test Patient....\n",
      "______________________________________________\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '../models/sig2sig_unetP02_r1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1998e073f82e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#train_for_patient(model_name, pat_num, epochs = epochs , run = run, input_size = 8000, train_path = training_data_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         stats_R, stats_S, stats_V = test_for_patient(model_name, pat_num, epochs = epochs , \n\u001b[1;32m---> 32\u001b[1;33m                                                         run = run, threshold = 0.1,input_size = 8000)\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#_______________________________________________\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\OneDrive - TUNI.fi\\Projects\\R-Peak-Detection-1D-CNN\\codes\\helper_functions.py\u001b[0m in \u001b[0;36mtest_for_patient\u001b[1;34m(model_name, pat_num, epochs, run, base_path, input_size, threshold)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msig2sig_unet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m     \u001b[0mpadded_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_windows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_test_windows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mecg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwin_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_windows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2202\u001b[0m           'first, then load the weights.')\n\u001b[0;32m   2203\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2204\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2205\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m'layer_names'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'model_weights'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2206\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '../models/sig2sig_unetP02_r1.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from helper_functions import train_for_patient, test_for_patient\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Epochs\n",
    "epochs = 30\n",
    "# Base folder path\n",
    "base_path = '../'\n",
    "training_data_path = base_path + 'training_data_aug/'\n",
    "model_name = 'sig2sig_unet' # or sig2sig_cnn\n",
    "\n",
    "all_pat = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "results_all = np.zeros((10,5), dtype = np.int32)\n",
    "perc_all = np.zeros((10,3), dtype = np.float32)\n",
    "results_S = np.zeros((7,4), dtype = np.int32)\n",
    "results_V = np.zeros((8,4), dtype = np.int32)\n",
    "s_count = 0\n",
    "v_count = 0\n",
    "\n",
    "for run in [1]:\n",
    "    \n",
    "    print('______________________________________________')\n",
    "    print('Run : ', str(run))\n",
    "    print('______________________________________________')\n",
    "    \n",
    "    for pat_num in all_pat:\n",
    "\n",
    "        train_for_patient(model_name, pat_num, epochs = epochs , run = run, input_size = 8000, train_path = training_data_path)\n",
    "        stats_R, stats_S, stats_V = test_for_patient(model_name, pat_num, epochs = epochs , \n",
    "                                                        run = run, threshold = 0.1,input_size = 8000)\n",
    "\n",
    "        #_______________________________________________\n",
    "        # Saving stats\n",
    "        #_______________________________________________\n",
    "        if stats_S != []:\n",
    "            results_S[s_count][0] = pat_num\n",
    "            results_S[s_count][1:] = stats_S[:3]\n",
    "\n",
    "            df_S = pd.DataFrame(results_S)\n",
    "            df_S.columns = ['Patient No', 'Total Beats', 'Detected', 'Missed']\n",
    "            f = base_path + 'Results/'+ model_name +'_S_r' + str(run) + '.csv'\n",
    "            df_S.to_csv (r'{}'.format(f), index = False, header=True)\n",
    "\n",
    "            s_count += 1\n",
    "        \n",
    "        if stats_V != []:\n",
    "            results_V[v_count][0] = pat_num\n",
    "            results_V[v_count][1:] = stats_V[:3]\n",
    "\n",
    "            df_V = pd.DataFrame(results_V)\n",
    "            df_V.columns = ['Patient No', 'Total Beats', 'Detected', 'Missed']\n",
    "            f = base_path + 'Results/'+ model_name +'_V_r' + str(run) + '.csv'\n",
    "            df_V.to_csv (r'{}'.format(f), index = False, header=True)\n",
    "\n",
    "            v_count += 1\n",
    "        \n",
    "        results_all[pat_num-1][0] = pat_num\n",
    "        results_all[pat_num-1][1:] = stats_R[:4]\n",
    "        perc_all[pat_num-1] = stats_R[4:7]\n",
    "\n",
    "\n",
    "\n",
    "        df_all = pd.DataFrame(results_all)\n",
    "        df_all = pd.concat([df_all, pd.DataFrame(perc_all, dtype = np.float32)], axis=1)\n",
    "        df_all.columns = ['Patient No', 'Total Beats', 'TP', 'FN', 'FP', 'Recall', 'Precision', 'F1']\n",
    "\n",
    "        if not os.path.exists(base_path + 'results/'):\n",
    "            os.makedirs(base_path + 'results/')\n",
    "\n",
    "        f = base_path + 'results/'+ model_name +'_all_r' + str(run) + '.csv'\n",
    "\n",
    "        df_all.to_csv (r'{}'.format(f), index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6ce3508e119d11a5597ec688043a166dd37850a4548418a299f7af683d14551"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
